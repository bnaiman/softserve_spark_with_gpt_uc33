{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "604e18f4-0f06-4e6b-8e7c-89222a2f9b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|\"id\";\"first_name\";\"last_name\";\"age\";\"country\"|\n",
      "+---------------------------------------------+\n",
      "|                         1;Darcy;Phillips;...|\n",
      "|                         2;Amelia;Wright;6...|\n",
      "|                          3;Haris;Ellis;61;CR|\n",
      "|                            4;Tony;Hall;51;JO|\n",
      "|                         5;Rubie;Stewart;2...|\n",
      "|                          6;Miley;Perry;27;ZA|\n",
      "|                         7;Marcus;Carter;6...|\n",
      "|                         8;Charlie;Harris;...|\n",
      "|                         9;Honey;Rogers;60;IL|\n",
      "|                         10;Luke;Harris;66;IR|\n",
      "|                         11;Spike;Murphy;5...|\n",
      "|                         12;Vincent;Adams;...|\n",
      "|                         13;James;Barnes;5...|\n",
      "|                         14;George;Bailey;...|\n",
      "|                         15;Sienna;Holmes;...|\n",
      "|                         16;Isabella;Ellio...|\n",
      "|                         17;Freddie;Martin...|\n",
      "|                         18;Kate;Wright;51;BE|\n",
      "|                         19;Albert;Myers;2...|\n",
      "|                         20;Connie;Wells;6...|\n",
      "+---------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CSV Reader\").getOrCreate()\n",
    "df_accounts = spark.read.csv(\"accounts.csv\", header=True, inferSchema=True)\n",
    "df_accounts.createOrReplaceTempView(\"accounts\")\n",
    "result1 = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM accounts \n",
    "\"\"\")\n",
    "\n",
    "result1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02e0ca0f-24bc-482b-a783-ff3085a4eafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+\n",
      "|\"id\";\"amount\";\"account_type\";\"transaction_date\";\"country\"|\n",
      "+---------------------------------------------------------+\n",
      "|                                     179528;-730.86;Bu...|\n",
      "|                                     378343;-946.98;Pe...|\n",
      "|                                     75450;7816.92;Pro...|\n",
      "|                                     357719;704.02;Bus...|\n",
      "|                                     110511;3462.6;Per...|\n",
      "|                                     461830;762.81;Pro...|\n",
      "|                                     30180;5390.24;Pro...|\n",
      "|                                     65398;4765.77;Per...|\n",
      "|                                     170899;8775.89;Bu...|\n",
      "|                                     234300;8455.18;Pr...|\n",
      "|                                     208027;6244.1;Bus...|\n",
      "|                                     161212;5904.56;Pe...|\n",
      "|                                     105372;4079.76;Pr...|\n",
      "|                                     205321;3570.4;Pro...|\n",
      "|                                     410863;2328.83;Bu...|\n",
      "|                                     486752;5454.8;Pro...|\n",
      "|                                     208564;8695.17;Pe...|\n",
      "|                                     196682;-905.87;Pe...|\n",
      "|                                     491196;8781.02;Pr...|\n",
      "|                                     108286;3485.95;Pe...|\n",
      "+---------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions = spark.read.csv(\"transactions.csv\", header=True, inferSchema=True)\n",
    "df_transactions.createOrReplaceTempView(\"transactions\")\n",
    "account_type_counts = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM transactions \n",
    "\"\"\")\n",
    "\n",
    "account_type_counts.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f936035b-2077-497b-9c02-ffc445dc65a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+----------------+-------+\n",
      "|    id| amount|account_type|transaction_date|country|\n",
      "+------+-------+------------+----------------+-------+\n",
      "|179528|-730.86|    Business|      2013-07-10|     SV|\n",
      "|378343|-946.98|    Personal|      2018-04-06|     YE|\n",
      "| 75450|7816.92|Professional|      2016-11-20|     SI|\n",
      "|357719| 704.02|    Business|      2016-11-06|     ID|\n",
      "|110511| 3462.6|    Personal|      2018-01-18|     BS|\n",
      "|461830| 762.81|Professional|      2017-06-20|     CN|\n",
      "| 30180|5390.24|Professional|      2021-05-26|     GN|\n",
      "| 65398|4765.77|    Personal|      2018-05-01|     TR|\n",
      "|170899|8775.89|    Business|      2013-10-16|     SK|\n",
      "|234300|8455.18|Professional|      2015-10-06|     LU|\n",
      "|208027| 6244.1|    Business|      2020-03-06|     AE|\n",
      "|161212|5904.56|    Personal|      2016-09-07|     EG|\n",
      "|105372|4079.76|Professional|      2015-02-12|     MT|\n",
      "|205321| 3570.4|Professional|      2012-07-02|     MU|\n",
      "|410863|2328.83|    Business|      2012-12-20|     SR|\n",
      "|486752| 5454.8|Professional|      2015-02-10|     CU|\n",
      "|208564|8695.17|    Personal|      2013-01-03|     IT|\n",
      "|196682|-905.87|    Personal|      2019-01-28|     HU|\n",
      "|491196|8781.02|Professional|      2017-05-11|     IR|\n",
      "|108286|3485.95|    Personal|      2011-12-13|     ZW|\n",
      "+------+-------+------------+----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions = spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"transactions.csv\")\n",
    "df_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96fdeca2-fa01-401a-814d-98ac3d09f87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|account_type|account_type_count|\n",
      "+------------+------------------+\n",
      "|    Personal|            481997|\n",
      "|Professional|            482170|\n",
      "|    Business|            482350|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.createOrReplaceTempView(\"transactions\")\n",
    "account_type_counts = spark.sql(\"\"\"\n",
    "    SELECT account_type, COUNT(DISTINCT id) as account_type_count \n",
    "    FROM transactions \n",
    "    GROUP BY account_type\n",
    "\"\"\")\n",
    "\n",
    "account_type_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d8d1cd6-a1fe-474e-b395-38e937d6a55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------+\n",
      "|    id|           balance|latest_date|\n",
      "+------+------------------+-----------+\n",
      "|482333|          27174.07| 2020-07-17|\n",
      "|222048|          48004.81| 2020-07-20|\n",
      "|328078|          36948.25| 2020-02-01|\n",
      "|192401|36736.979999999996| 2020-01-30|\n",
      "|273916| 47475.37999999999| 2021-05-30|\n",
      "|485103|          62198.93| 2021-05-22|\n",
      "|300282|          55103.62| 2021-05-01|\n",
      "| 20683| 56448.72000000001| 2021-10-27|\n",
      "| 15846|58671.909999999996| 2020-12-23|\n",
      "|446783|          98085.51| 2021-12-11|\n",
      "| 92182|           42335.3| 2020-08-08|\n",
      "|477485|          22114.03| 2020-05-23|\n",
      "|171142|40428.899999999994| 2021-04-07|\n",
      "|317762|          40025.55| 2021-12-02|\n",
      "| 65478|           57941.9| 2021-10-06|\n",
      "|306768|          26566.93| 2019-12-19|\n",
      "|380411|          43652.94| 2020-06-02|\n",
      "|304681|          37827.69| 2021-03-26|\n",
      "|475638|44509.100000000006| 2021-11-23|\n",
      "| 97413|          39611.24| 2018-05-01|\n",
      "+------+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT id, \n",
    "           SUM(amount) as balance, \n",
    "           MAX(transaction_date) as latest_date \n",
    "    FROM transactions \n",
    "    GROUP BY id\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59c2acc6-aef2-420f-acac-a9640ec311ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+------------------+------------------+--------+--------+------------------+------------------+-------+------------------+------------------+------------------+\n",
      "|       full_name|    2011|              2012|              2013|    2014|    2015|              2016|              2017|   2018|              2019|              2020|              2021|\n",
      "+----------------+--------+------------------+------------------+--------+--------+------------------+------------------+-------+------------------+------------------+------------------+\n",
      "|   Connie Gibson|     0.0|           8486.84|           3664.69| 7385.69|     0.0|           1717.48|           2436.81|    0.0|           5545.76|               0.0|               0.0|\n",
      "|    Paige Taylor|     0.0|               0.0|           5618.18|     0.0|     0.0|           2784.89|           2634.62|    0.0|           9963.01|          14458.31|            436.22|\n",
      "|  Adison Douglas|     0.0|          10622.48|          14322.94|     0.0|     0.0|               0.0|           6328.92|8248.97|               0.0|               0.0|           3406.01|\n",
      "|    Arnold Kelly|     0.0|           8771.92|               0.0|12260.09| 2920.82|           1359.71|               0.0|6084.71|               0.0|               0.0|           9068.72|\n",
      "|   Justin Cooper|20224.62|           5977.15|               0.0|16800.75| 6475.07|               0.0|               0.0|2582.08|           5888.01|            1988.9|               0.0|\n",
      "| Kirsten Stevens| 1008.24|               0.0|            366.76| 8655.74|     0.0|           1080.08|               0.0|    0.0|               0.0|           9788.55|               0.0|\n",
      "|     Amber Evans|     0.0|           3798.16|               0.0|     0.0|     0.0|13363.730000000001|           7020.42|6696.24|            942.82|11777.369999999999|               0.0|\n",
      "|   Arnold Morris|     0.0|               0.0|               0.0|12931.32| 2026.07|               0.0|           2641.35|    0.0|           3427.39|               0.0|           8965.33|\n",
      "|  Dale Alexander| 9022.91|           9812.92|               0.0| 2848.98|     0.0|          10517.91|               0.0|3863.86|               0.0|          20184.66|13121.400000000001|\n",
      "|   Darcy Edwards|     0.0|               0.0|               0.0|16848.72|     0.0|           7922.11|10337.599999999999|    0.0|           3722.62|            7545.8|               0.0|\n",
      "|Charlie Anderson|  6809.2|           5100.17|               0.0|12423.34| 5351.78|           6502.27|               0.0|    0.0|24717.589999999997|               0.0|           6316.73|\n",
      "|    April Harper|  7841.0|           13925.4|           1128.23|     0.0|14423.01|           8253.21|               0.0|    0.0|               0.0|          15071.61|               0.0|\n",
      "|     Blake Moore|     0.0| 7885.150000000001|               0.0|15699.94|     0.0|           2882.76|           6943.04|3428.67|               0.0|           7861.35|           8448.15|\n",
      "|      Clark Hall|     0.0|           1784.62|               0.0|     0.0|     0.0|           2125.18|               0.0|    0.0|           6097.56|           8677.36|          12687.51|\n",
      "|  Stuart Hawkins|     0.0|               0.0|           1279.27|     0.0|     0.0|           1453.43|               0.0|6499.72|               0.0|           2035.15|11150.640000000001|\n",
      "|  Kelsey Spencer| 8000.61|14881.710000000001|           9079.28| 6572.99|     0.0|           7699.19|               0.0| 4867.4|               0.0|               0.0|               0.0|\n",
      "|  Olivia Hawkins| 8112.16|           1196.61|           7345.82|  576.65|     0.0|             82.74|               0.0|    0.0|           1244.63|               0.0|           4903.52|\n",
      "|      Daryl Reed| 8236.85|               0.0|               0.0|     0.0| 3755.61|            1529.8|               0.0|8891.51|               0.0|               0.0|           9841.84|\n",
      "|   Michael Smith| 10089.7|           5472.81|               0.0| 14594.2|     0.0|           3695.08|               0.0|8645.05|           8809.05|           2365.13|           6361.37|\n",
      "|    Julia Bailey| 21385.1|           8524.32|15236.189999999999| 3997.32|   60.17|          10723.63|           3713.31|    0.0|               0.0|          14140.26|13086.829999999998|\n",
      "+----------------+--------+------------------+------------------+--------+--------+------------------+------------------+-------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, year, concat_ws\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SwissEarningsCalculation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the datasets from the current folder with semicolon delimiters and headers\n",
    "accounts = spark.read.csv(\"accounts.csv\", header=True, sep=\";\", inferSchema=True)\n",
    "transactions = spark.read.csv(\"transactions.csv\", header=True, sep=\";\", inferSchema=True)\n",
    "country_abbreviations = spark.read.csv(\"country_abbreviation.csv\", header=True, sep=\";\", inferSchema=True)\n",
    "\n",
    "# Columns based on your descriptions:\n",
    "# accounts: id, first_name, last_name, country\n",
    "# transactions: id, amount, transaction_date\n",
    "# country_abbreviations: abbreviation, country_full_name\n",
    "\n",
    "# Filter Swiss users\n",
    "swiss_accounts = accounts.join(country_abbreviations, accounts.country == country_abbreviations.abbreviation) \\\n",
    "    .filter(col(\"country_full_name\") == \"Switzerland\")\n",
    "\n",
    "# Filter transactions with positive value and extract year\n",
    "transactions = transactions.filter(col(\"amount\") > 0) \\\n",
    "    .withColumn(\"year\", year(col(\"transaction_date\")))\n",
    "\n",
    "# Join accounts with transactions using \"id\" column\n",
    "joined_data = transactions.join(swiss_accounts, transactions.id == swiss_accounts.id)\n",
    "\n",
    "# Group by user full name and year to aggregate earnings\n",
    "aggregated_data = joined_data.groupBy(\n",
    "    concat_ws(\" \", col(\"first_name\"), col(\"last_name\")).alias(\"full_name\"),\n",
    "    \"year\"\n",
    ").agg(sum(\"amount\").alias(\"earnings\"))\n",
    "\n",
    "# Pivot table by year\n",
    "pivot_table = aggregated_data.groupBy(\"full_name\").pivot(\"year\").sum(\"earnings\").na.fill(0)\n",
    "\n",
    "pivot_table.show()\n",
    "\n",
    "# Stop the spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af8bc6c3-475d-4596-ba63-a735ffe78f12",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrameReader.csv() got an unexpected keyword argument 'delimiter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transactions_with_level\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Load the transactions dataset with ';' delimiter and header\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m transactions \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransactions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Process the dataset\u001b[39;00m\n\u001b[1;32m     29\u001b[0m result \u001b[38;5;241m=\u001b[39m assign_transaction_level(transactions)\n",
      "\u001b[0;31mTypeError\u001b[0m: DataFrameReader.csv() got an unexpected keyword argument 'delimiter'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, percentile_approx\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Transaction Levels\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def assign_transaction_level(transactions):\n",
    "    # Calculate the 25th and 75th percentiles of the 'amount' column\n",
    "    percentiles = transactions.approxQuantile(\"amount\", [0.25, 0.75], 0.01)\n",
    "    low_threshold, high_threshold = percentiles\n",
    "\n",
    "    # Add the 'level' column based on the 'amount' column\n",
    "    transactions_with_level = transactions.withColumn(\n",
    "        \"level\",\n",
    "        when(col(\"amount\") >= high_threshold, \"high\")\n",
    "        .when((col(\"amount\") < high_threshold) & (col(\"amount\") >= low_threshold), \"average\")\n",
    "        .otherwise(\"low\")\n",
    "    )\n",
    "\n",
    "    return transactions_with_level\n",
    "\n",
    "# Load the transactions dataset with ';' delimiter and header\n",
    "transactions = spark.read.csv(\"transactions.csv\", header=True, inferSchema=True, delimiter=';')\n",
    "\n",
    "# Process the dataset\n",
    "result = assign_transaction_level(transactions)\n",
    "result.show()\n",
    "\n",
    "# Optionally, stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4de1c0a6-08d1-47b3-b3b2-6fc986b10908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+----------------+-------+-------+\n",
      "|    id| amount|account_type|transaction_date|country|  level|\n",
      "+------+-------+------------+----------------+-------+-------+\n",
      "|179528|-730.86|    Business|      2013-07-10|     SV|    low|\n",
      "|378343|-946.98|    Personal|      2018-04-06|     YE|    low|\n",
      "| 75450|7816.92|Professional|      2016-11-20|     SI|   high|\n",
      "|357719| 704.02|    Business|      2016-11-06|     ID|    low|\n",
      "|110511| 3462.6|    Personal|      2018-01-18|     BS|average|\n",
      "|461830| 762.81|Professional|      2017-06-20|     CN|    low|\n",
      "| 30180|5390.24|Professional|      2021-05-26|     GN|average|\n",
      "| 65398|4765.77|    Personal|      2018-05-01|     TR|average|\n",
      "|170899|8775.89|    Business|      2013-10-16|     SK|   high|\n",
      "|234300|8455.18|Professional|      2015-10-06|     LU|   high|\n",
      "|208027| 6244.1|    Business|      2020-03-06|     AE|average|\n",
      "|161212|5904.56|    Personal|      2016-09-07|     EG|average|\n",
      "|105372|4079.76|Professional|      2015-02-12|     MT|average|\n",
      "|205321| 3570.4|Professional|      2012-07-02|     MU|average|\n",
      "|410863|2328.83|    Business|      2012-12-20|     SR|average|\n",
      "|486752| 5454.8|Professional|      2015-02-10|     CU|average|\n",
      "|208564|8695.17|    Personal|      2013-01-03|     IT|   high|\n",
      "|196682|-905.87|    Personal|      2019-01-28|     HU|    low|\n",
      "|491196|8781.02|Professional|      2017-05-11|     IR|   high|\n",
      "|108286|3485.95|    Personal|      2011-12-13|     ZW|average|\n",
      "+------+-------+------------+----------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, percentile_approx\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Transaction Levels\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def assign_transaction_level(transactions):\n",
    "    # Calculate the 25th and 75th percentiles of the 'amount' column\n",
    "    percentiles = transactions.approxQuantile(\"amount\", [0.25, 0.75], 0.01)\n",
    "    low_threshold, high_threshold = percentiles\n",
    "\n",
    "    # Add the 'level' column based on the 'amount' column\n",
    "    transactions_with_level = transactions.withColumn(\n",
    "        \"level\",\n",
    "        when(col(\"amount\") >= high_threshold, \"high\")\n",
    "        .when((col(\"amount\") < high_threshold) & (col(\"amount\") >= low_threshold), \"average\")\n",
    "        .otherwise(\"low\")\n",
    "    )\n",
    "\n",
    "    return transactions_with_level\n",
    "\n",
    "# Load the transactions dataset with ';' delimiter and header\n",
    "transactions = spark.read.option(\"delimiter\", \";\").csv(\"transactions.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Process the dataset\n",
    "result = assign_transaction_level(transactions)\n",
    "result.show()\n",
    "\n",
    "# Optionally, stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e674568-4778-464d-8d0e-213ef1439b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
